#!/bin/bash

# Personnal aliases
alias edssh='code ${HOME}/.ssh/config'
alias rea='. ${HOME}/.zsh_aliases && . ${HOME}/.zshenv'
alias eda='code ${HOME}/.zsh_aliases'
alias ede='code ${HOME}/.zshenv'
alias edc='code ${HOME}/.zsh_custom'
alias eds='code ${HOME}/.config/starship.toml'
alias edd='code $(devbox global path)/devbox.json'

alias connect='proxy_start && update_vscode_jenkins_plugin_token && sfa'

alias c='code'
alias cr='code -r'

alias dr='devbox run'
alias tt='devbox run test'

alias get_arti_log='scp repo.tooling.prod.cdsf.io:/data/artifactory/log/artifactory-service.log .'

GOPATH=$(go env GOPATH)
export PATH="$PATH:$GOPATH/bin"

alias sc='search_code'
function search_code() {
  grep -I -R "$1" "${HOME}/dev"
}

alias findz="find ./* -type f | fzf --preview 'bat --color=always {}' --preview-window '~3'"

# https://github.com/ryanoasis/nerd-fonts/releases/download/v3.2.1/FiraCode.zip
function install_fonts() {
  font_url=$1
  if [[ $font_url == "" ]]
  then
    echo "${RED}[Error] missing parameter : font URL"
    return 1
  fi
  font_name=${font_url##*/} && \
    wget "${font_url}" && \
    unzip "${font_name}" -d ~/.fonts && \
    fc-cache -fv
}

function update() {
  devbox version update
  eval "$(devbox global shellenv --recompute)"
  refresh-global
  devbox global update
  [[ -f "./devbox.json" ]] && devbox update
  refresh-global
  update_aws_sso_cli
  kubectl krew upgrade
}

function get_last_addon_template() {
  cp "${HOME}/dev/oam.k8s.addons.addon-template/."* .
  cp -r "${HOME}/dev/oam.k8s.addons.addon-template/"* .
  rm ./oam_k8s_addons_addon-template_export.tar.gz
}

function backup() {
  sudo cp -Lr "${HOME}/.zsh_history" "${HOME}/.zshenv" "${HOME}/.zsh_custom" "${HOME}/.zsh_aliases" \
    "${HOME}/.config/starship.toml" "${HOME}/.gitconfig" \
    "${HOME}/.ssh" "${HOME}/.gpg" "${HOME}/.aws" "${HOME}/.config/google-chrome/Default/Bookmarks" \
    /etc/cntlm.conf /etc/redsocks.conf /usr/local/sbin/redsocks-iptables "$(devbox global path)/devbox.json" \
    /media/sf_sharedfolder
  sudo mkdir -p /media/sf_sharedfolder/.kube
  sudo cp -r "${HOME}/.kube/kubeconfig"* "/media/sf_sharedfolder/.kube"

  # copy in my github repo
  sudo cp "${HOME}/.zsh_custom" "${HOME}/.zsh_aliases" \
    "${HOME}/.config/starship.toml" "$(devbox global path)/devbox.json" \
    "${HOME}/dev/devbox-global"
  pushd "${HOME}/dev/devbox-global" > /dev/null || return
  git add .zsh_custom .zsh_aliases starship.toml > /dev/null
  git commit -m "Update config files" > /dev/null
  git add devbox.json > /dev/null
  git commit -m "Update packages list" > /dev/null
  git push
  popd > /dev/null 2>&1 || true
}

function free_space() {
  echo "------------------------------------------------"
  echo "Free space"
  echo "------------------------------------------------"
  echo "Disk space before cleaning :"
  df -h /dev/sda2
  echo "------------------------------------------------"
  sudo apt-get -y clean
  sudo apt -y autoclean
  sudo apt-get -y autoremove --purge
  [[ -d "${HOME}/.cache/cloud-code/installer" ]] && rm -rf "${HOME}/.cache/cloud-code/installer"
  # go clean -cache -modcache -i -r
  [[ -d "/var/lib/snapd/cache" ]] && rm -rf /var/lib/snapd/cache/*
  sudo journalctl --rotate
  sudo journalctl --vacuum-time=1s > /dev/null 2>&1
  docker system prune -a -f --volumes
  for item in $(find "${HOME}" -name ".terraform" -type d | grep -E "^/home"); do rm -rf "$item"; done
  # nix-store --gc
  echo "------------------------------------------------"
  echo "Disk space after cleaning :"
  df -h /dev/sda2
  echo "------------------------------------------------"
}

########################################
#                Jenkins
########################################
alias ut='update_vscode_jenkins_plugin_token'
function configure_vscode_jenkins_plugin_in_all_repos() {
  for git_folder in "${HOME}/dev/"*/.git
  do
    [[ -d "$git_folder" ]] || return

    configure_vscode_jenkins_plugin "${git_folder%/.git}" false
  done
}
function configure_vscode_jenkins_plugin() {
  git_root_folder=$1
  [[ $git_root_folder == "" ]] && git_root_folder=$(pwd)
  # skip repos without Jenkinsfile
  [[ -f "${git_root_folder}/Jenkinsfile" ]] || return
  echo "=> Configure VScode jenkins plugin for ${git_root_folder##*/}"
  pushd "${git_root_folder}" > /dev/null || return
  git_url=$(git remote get-url origin)
  main_branch_name=$(get_main_branch_name)
  popd > /dev/null 2>&1 || true
  git_project_name=$(echo "$git_url" | perl -lne 'print $1 if /git@.+:(.+)\.git/')
  if [[ $git_project_name == "" ]]
  then
    echo -e "${RED}Error, malformed git URL : '$git_url' don't match 'git@xxx:xxx.git'"
    return 1
  fi
  jenkins_organization_name="$(echo "${git_project_name%%/*}" | tr '[:lower:]' '[:upper:]')"
  cat <<EOF >"${git_root_folder}/.jenkins"
{
"url": "https://jenkins.tooling.prod.cdsf.io/job/${jenkins_organization_name}/job/${git_project_name//\//%2F}/job/${main_branch_name}/",
"username": "uia59190",
"password": "xxx"
}
EOF
  echo "OK"

  if ! grep '.jenkins' "${git_root_folder}/.gitignore" > /dev/null
  then
    echo "Configure gitignore"
    cat <<EOF >>"${git_root_folder}/.gitignore"

# Jenkins status VScode plugin configuration file, don't commit it because it contains your Jenkins personal token !
.jenkins

EOF
    pushd "${git_root_folder}" > /dev/null || return
    git add .gitignore > /dev/null
    git commit -m "Ignore VScode jenkins plugin config file" > /dev/null
    popd > /dev/null 2>&1 || true
  fi
  [[ $2 != "false" ]] && update_vscode_jenkins_plugin_token ""
}

function update_vscode_jenkins_plugin_token() {
  new_token=$1
  if [[ -z $new_token ]]
  then
    # shellcheck disable=SC1091
    source "$HOME/.ssh/jenkins.sh"
    result=$("${HOME}/dev/oam.ci.jenkins-automation/create-personal-token.sh" -q --hostname "jenkins.tooling.prod.cdsf.io" --username "${JENKINS_GTP_SA_USER}" -n "VSCode_plugin")
  fi
  new_token=$(echo "$result" | tr -d '\n')
  if ! [[ $new_token =~ ^[0-9a-z]+$ ]] 
  then
    echo "[ERROR] $result"
  else
    export JENKINS_GTP_TOKEN="$new_token"
    nb=0
    for git_folder in "${HOME}/dev/"*/.git
    do
      [[ -d "$git_folder" ]] || continue

      git_root_folder="${git_folder%/.git}"
      if [[ -f "${git_root_folder}/.jenkins" ]]
      then
        sed -i "s/\"password\": \".*\"/\"password\": \"${new_token}\"/" "${git_root_folder}/.jenkins"
        nb=$((nb+1))
      fi
    done
    echo "$nb tokens updated under \"${HOME}/dev\""
  fi
}

function allow_jenkins_slave_ssh() {
  ssh admin@jenkins.tooling.prod.cdsf.io << EOF
sudo sed -i 's/^AllowTcpForwarding.*/AllowTcpForwarding yes/' /etc/ssh/sshd_config
sudo systemctl restart sshd
EOF
}

function trigger_jenkins_scan() {
  git_root_folder=$1
  [[ $git_root_folder == "" ]] && git_root_folder=$(pwd)
  if [[ ! -f "$git_root_folder/.jenkins" ]];
  then
    echo -e "${RED}Error, '.jenkins' config file not found in $git_root_folder"
    return 1
  fi
  url=$(jq -r '.url' "$git_root_folder/.jenkins")
  job_root_url="${url%/job/*/}"
  username=$(jq -r '.username' "$git_root_folder/.jenkins")
  password=$(jq -r '.password' "$git_root_folder/.jenkins")
  curl -u "$username:$password" -X POST "$job_root_url/build"
}

function fix_jenkins_vscode_plugin() {
  for file in "${HOME}/.vscode/extensions/alefragnani.jenkins-status-"*"/dist/extension.js"
  do
    if [[ ! -f "${file}.backup" ]]
    then
      cp "$file" "${file}.backup"
    fi
    echo '==> Remove 3 occurrence of "o.Uri.parse(" in the file (don'\''t forget to remove also the corresponding closing bracket)'
    echo '==> Replace .openInJenkinsConsoleOutput",(()=>{c.getStatus(p,u,l).then((e=>{e.connectionStatus===o.ConnectionStatus.Connected?r.env.openExternal(this.settingNameToUrl[s.name]+e.buildNr.toString()+"/console'
    echo 'by .openInJenkinsConsoleOutput",(()=>{c.getStatus(p,u,l).then((e=>{e.connectionStatus===o.ConnectionStatus.Connected?r.env.openExternal(this.settingNameToUrl[s.name].replaceAll('\''%2F'\'','\''%252F'\'').replace(/\/job\/([^\/]+)\/job\/([^\/]+)\/job\//, '\''/blue/organizations/jenkins/$1%2F$2/detail/'\'')+e.buildNr.toString()'
    echo '==> Replace this.statusBarItems[s.name].name=t,this.statusBarItems[s.name].command="Jenkins."+s.name+".openInJenkins"'
    echo 'by this.statusBarItems[s.name].name=t,this.statusBarItems[s.name].command="Jenkins."+s.name+".openInJenkinsConsoleOutput"'
    echo 'close the file and restart VS Code'
    code "$file"
  done
}

########################################
#                Git
########################################
alias gicp='git cherry-pick'
alias gil='git log --graph --oneline --color --decorate'
alias gir="git_rebase_origin"
alias gipc="git_propagate_commits"
alias giri='git rebase -i '
alias girst='git reset --hard '
alias gip='git update-index --chmod=+x '
alias giap='git commit --amend --no-edit && git push -f'
alias gidb='deleteOldBranches'
alias girsto='git_reset_to_origin'
alias girc='GIT_EDITOR=true git rebase --continue'
alias gdb='delete_git_branch'
alias cc='clonecode'

function delete_git_branch() {
  if [[ $1 != "" ]]
  then
    git push --delete origin $1
  fi
  deleteOldBranches
}

function archive_old_git_branches() {
  GITLAB_HOSTNAME=git.int.be.continental.cloud PRIVATE_TOKEN=$GITLAB_DEV_PRIVATE_TOKEN "$HOME/dev/oam.ci.gitlab-automation/archive_old_git_branches.sh" -g 2 --auto-approve
  GITLAB_HOSTNAME=code.tooling.prod.cdsf.io PRIVATE_TOKEN=$GITLAB_GTP_PRIVATE_TOKEN "$HOME/dev/oam.ci.gitlab-automation/archive_old_git_branches.sh" -g 114 --auto-approve
  GITLAB_HOSTNAME=code.tooling.prod.cdsf.io PRIVATE_TOKEN=$GITLAB_GTP_PRIVATE_TOKEN "$HOME/dev/oam.ci.gitlab-automation/archive_old_git_branches.sh" -g 5 --auto-approve
  # GITLAB_HOSTNAME=infra.int.be.continental.cloud PRIVATE_TOKEN=$GITLAB_INFRA_PRIVATE_TOKEN $HOME/dev/oam.ci.gitlab-automation/archive_old_git_branches.sh -g 25 --auto-approve
  # GITLAB_HOSTNAME=infra.int.be.continental.cloud PRIVATE_TOKEN=$GITLAB_INFRA_PRIVATE_TOKEN $HOME/dev/oam.ci.gitlab-automation/archive_old_git_branches.sh -g 6 --auto-approve
}

function close_git_branch() {
  source_branch=$1
  if [[ -z $source_branch ]]
  then
    source_branch=$(get_main_branch_name)
  fi
  git_rebase_origin "$source_branch"
  current_branch=$(git rev-parse --abbrev-ref HEAD)
  git checkout "$source_branch"
  git pull
  git merge "$current_branch"
  git branch -D "$current_branch"
  git push --delete origin "$current_branch"
}

function git_propagate_commits() {
  target_branch=$1
  if [[ -z $target_branch ]]
  then
    echo "${RED}[ERROR] target_branch is required"
    return 1
  fi
  current_branch=$(git rev-parse --abbrev-ref HEAD)
  git checkout "$target_branch" && \
    git pull && \
    git rebase "$current_branch" && \
    if [[ $2 == "" ]]
    then
      git push -f && \
      git checkout "$current_branch"
    fi
}

function git_rebase_origin() {
  source_branch=$1
  if [[ -z $source_branch ]]
  then
    source_branch=$(get_main_branch_name)
  fi
  git fetch && git rebase "origin/$source_branch"
}

function get_main_branch_name() {
  if [[ ! -z $(git ls-remote --heads origin master) ]];
  then
    echo "master"
  else
    echo "main"
  fi
}

function git_reset_to_origin() {
  current_branch=$(git rev-parse --abbrev-ref HEAD)
  if [[ $current_branch != "" ]];
  then
    git reset --hard "origin/$current_branch"
  fi
}

function deleteOldBranches() {
  git fetch -p
  git branch -vv | grep ': gone]'| grep -v "\*" | awk '{ print $1; }' | xargs -r git branch -D
  branches=$(git branch -a)
  echo "$branches"
}

function deleteOldBranchesInEachRepo() {
  for git_folder in "${HOME}/dev/"*/.git
  do
    [[ -d "$git_folder" ]] || continue
    cd "${git_folder%/.git}" && deleteOldBranches
  done
}

function clonecode() {
  git_url=$1
  target_dir=$(echo "$git_url" | perl -lne 'print $1 if /git@.+:(.+)\.git/' | tr '/' '.')
  if [[ "$target_dir" == "" ]]
  then
    target_dir=$(echo "$git_url" | perl -lne 'print $1 if /https:\/\/.+?\/(.+)\.git/' | tr '/' '.')
  fi
  if [[ "$target_dir" == "" ]]
  then
    echo -e "${RED}Error, malformed git URL : '$git_url' don't match 'git@xxx:xxx.git' nor 'https://xxx/xxx.git'"
    return 1
  fi
  if [[ ! -d "${HOME}/dev/$target_dir" ]]
  then
    echo "=> Git clone \"$target_dir\""
    git clone "$git_url" "${HOME}/dev/$target_dir"
  fi
  echo "=> Open \"$target_dir\" in VScode"
  code "${HOME}/dev/$target_dir"
  configure_vscode_jenkins_plugin "${HOME}/dev/$target_dir"
}


function custom_rebase() {
  if [[ "$1" == "" ]];
  then
    echo -e "${RED}Error, one argument required : git ref of new desired base"
    return 1
  fi
  tmp=$(mktemp)
  git archive --format=tar HEAD > "$tmp"
  git reset --hard $1
  if [[ "$2" == "-d" ]];
  then
    rm -rf *
  fi
  tar xf "$tmp"
  rm -f "$tmp"
}


############################################################
#                          Proxy
############################################################

DEFAULT="\e[39m"
GREEN="\e[32m"
RED="\e[31m"

alias cls='clock_sync'
function clock_sync() {
  sudo /usr/sbin/VBoxService --timesync-set-start --timesync-set-on-restore --timesync-set-threshold 1000 --timesync-interval 5000
  return
}

# Check internet access, fail after 15s
function check_internet_connection() {
  if [[ "$(curl -m 5 -L -s -o /dev/null -I -w '%{http_code}' http://www.google.fr)" == "200" ]]
  then
    echo -e "${GREEN}OK"
    return 0
  else
    echo -e "${RED}KO"
    return 1
  fi
}

# Check corporate access, fail after 15s
function check_corporate_connection() {
  if [[ "$(curl -m 5 -L -s -o /dev/null -I -w '%{http_code}' https://repo.int.be.continental.cloud)" == "200" ]]
  then
    echo -e "${GREEN}OK"
    return 0
  else
    echo -e "${RED}KO"
    return 1
  fi
}

# Check DNS
function check_dns() {
  if dig +short www.google.fr +time=5 +tries=3 > /dev/null
  then
    echo -e "${GREEN}OK"
    return 0
  else
    echo -e "${RED}KO"
    return 1
  fi
}

alias prs='proxy_status'
function proxy_status() {
  echo -en "${DEFAULT}CNTLM : ${GREEN}"; sudo systemctl is-active cntlm.service
  echo -en "${DEFAULT}REDSOCKS : ${GREEN}"; sudo systemctl is-active redsocks.service
  echo -en "${DEFAULT}INTERNET CONNECTION : "; check_internet_connection
  echo -en "${DEFAULT}CORPORATE_CONNECTION : "; check_corporate_connection
  echo -en "${DEFAULT}DNS_CONNECTION : "; check_dns
  echo -en "${DEFAULT}"
}

# Proxy management
alias proxy_unset='unset http{s,}_proxy && unset HTTP{S,}_PROXY && unset FTP_PROXY && unset ftp_proxy && unset ALL_PROXY && unset all_proxy'
alias proxy_start='sudo systemctl daemon-reload && sudo systemctl start cntlm.service && sudo systemctl start redsocks.service; sleep 2; proxy_status'
alias proxy_stop='sudo systemctl stop redsocks.service cntlm.service; sleep 2; proxy_status'

alias rep='proxy_restart'
function proxy_restart() {
  sudo systemctl daemon-reload
  sudo systemctl restart cntlm 
  sudo systemctl restart redsocks 
  sudo systemctl restart systemd-resolved
  sleep 2
  proxy_status
}

alias red='dns_restart'
function dns_restart() {
  sudo systemctl daemon-reload
  sudo systemctl restart systemd-resolved
  sleep 5
  proxy_status
}

function cntlm_hash_passwd() {
  cntlm -u uia59190@cw01 -H
  echo "sudo gedit /etc/cntlm.conf"
}

########################################
#             Docker
########################################
function docker_prune() {
    docker container prune
    docker image prune
}
function docker_run() {
    docker run --rm -it --network=host -u root -v "$(pwd):/current" -w /current --entrypoint "" -v "${HOME}/.ssh:/root/.ssh" -v "${HOME}/.aws:/root/.aws" -e AWS_PROFILE -e AWS_ACCESS_KEY_ID -e AWS_SECRET_ACCESS_KEY -e AWS_DEFAULT_REGION -e ANSIBLE_SSH_ARGS="-F /dev/null" "$@"
}
function docker_bash() {
    docker_run "$1" bash
}
function docker_sh() {
    docker_run "$1" sh
}
function docker_enter() {
    docker_bash $1 || docker_sh $1
}

alias dop='docker_prune'
alias dor='docker_run'
alias dob='docker_bash'
alias dos='docker_sh'
alias doi='docker images'
alias doe='docker_enter'

alias push_to_all_ecr='$HOME/dev/oam.builds.jenkins.shared-lib.common/resources/gtp/push_to_ecr.sh -e all -i '

# Portainer
alias portainer='docker run -d -p 9001:9000 --name portainer -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer'
alias portainer_stop='docker rm -f portainer'

########################################
#                 K8S
########################################
# shellcheck disable=SC1090
source <(kubectl completion zsh)

# alias kon='kubeon'
alias kns='kubectl ns'
# alias kcx='kcx'
alias kcx='set_kubeconfigs && kubectl ctx'
alias kga='kubectl get all'
alias kgn='kubectl get nodes'
alias kgp='kubectl get po'
alias kgd='kubectl get deploy'
alias kgap='kubectl get app -n argocd'
alias kgas='kubectl get appset -n argocd'
alias kg='kubectl get'
alias kgy='kubectl get -o yaml'
alias kgj='kubectl get -o json'
alias kd='kubectl describe'
alias kdr='kubectl describe rollout'
alias kda='kubectl describe analysisruns'
alias klf='kubectl logs -f --tail 100'
alias kkyv='kubectl describe polr | grep "Result: \+fail" -B10'
alias kgpf='kubectl get po -A | grep -v Running | grep -v Completed'
alias kgpfw='kubectl get po -A -o wide | grep -v Running | grep -v Completed'
alias kgps='kubectl get pods -A -o wide --field-selector spec.nodeName='
alias kgi='kubectl get pods -o jsonpath="{..image}" | tr -s "[[:space:]]" "\n" | sort | uniq -c'
alias kgia='kubectl get pods -A -o jsonpath="{..image}" | tr -s "[[:space:]]" "\n" | sort | uniq -c'
alias kgda="get_po_disruption_issue"

alias argocdpf='argocd --plaintext --grpc-web --insecure --port-forward --port-forward-namespace argocd'

alias kindkc='export KUBECONFIG="$HOME/.kind/config" && kind get kubeconfig > "$HOME/.kind/config"'
# Clusters manager EKS
alias kcm='kcx tls2_CDSF_Services_Framework_Admin_cicd@clusters-manager-PROD'
alias kcmd='${HOME}/dev/oam.ci.cluster-api-manager/scripts/get_clusters-manager_kubeconfig.sh -c DEV && kcx tls2_CDSF_Services_Framework_Admin_cicd@clusters-manager-DEV'
alias kcmt='${HOME}/dev/oam.ci.cluster-api-manager/scripts/get_clusters-manager_kubeconfig.sh -c TEST && kcx tls2_CDSF_Services_Framework_Admin_cicd@clusters-manager-TEST'
# Harbor EKS
alias kch='kcx tls2_CDSF_Services_Framework_Admin_cicd@prod-registry-cluster && kns harbor'
alias kchd='get_harbor_eks_kubeconfig.sh -c dev && kcx tls2_CDSF_Services_Framework_Admin_cicd@dev-registry-cluster'
alias kcht='get_harbor_eks_kubeconfig.sh -c test && kcx tls2_CDSF_Services_Framework_Admin_cicd@test-registry-cluster'
# Applicative K8S PROD
alias kci='kcx tls2_CDSF_Services_Framework_Admin_k8s_int@k8s-integration-eu-central-1'
alias kcv='kcx tls2_CDSF_Services_Framework_Admin_k8s_val@k8s-validation-eu-central-1'
alias kcp='kcx tls2_CDSF_Services_Framework_Admin_k8s_prod_eu@k8s-production-eu-central-1'
alias kcw='kcx tls2_CDSF_Services_Framework_Admin_k8s_prod_euw@k8s-production-eu-west-1'
alias kcu='kcx tls2_CDSF_Services_Framework_Admin_k8s_prod_us@k8s-production-us-east-1'
alias kcc='kcx CDSF_Services_Framework_Admin_k8s_prod_cn@k8s-production-cn-north-1'
# Applicative K8S TEST & DEV
alias kcid='get_applicative_k8s_kubeconfig.sh -c dev -e int && kcx tls2_CDSF_Services_Framework_Admin_k8s_int_dev@k8s-integration-dev-eu-central-1'
alias kcit='get_applicative_k8s_kubeconfig.sh -c test -e int && kcx tls2_CDSF_Services_Framework_Admin_k8s_int_test@k8s-integration-test-eu-central-1'
alias kcvd='get_applicative_k8s_kubeconfig.sh -c dev -e val && kcx tls2_CDSF_Services_Framework_Admin_k8s_val_dev@k8s-validation-dev-eu-central-1'
alias kcvt='get_applicative_k8s_kubeconfig.sh -c test -e val && kcx tls2_CDSF_Services_Framework_Admin_k8s_val_test@k8s-validation-test-eu-central-1'
# API gateway EKS K8S PROD
alias kcai='kcx tls2_CDSF_Services_Framework_Admin_apigtw_int@apigw-integration-eu-central-1'
alias kcav='kcx tls2_CDSF_Services_Framework_Admin_apigtw_val@apigw-validation-eu-central-1'
alias kcap='kcx tls2_CDSF_Services_Framework_Admin_apigtw_prod_eu@apigw-production-eu-central-1'
alias kcaw='kcx tls2_CDSF_Services_Framework_Admin_apigtw_prod_euw@apigw-production-eu-west-1'
alias kcau='kcx tls2_CDSF_Services_Framework_Admin_apigtw_prod_us@apigw-production-us-east-1'
alias kcac='kcx CDSF_Services_Framework_Admin_apigtw_prod_cn@apigw-production-cn-north-1'
# API gateway EKS TEST & DEV
alias kcaid='get_apigw_kubeconfig.sh -c dev -e int && kcx tls2_CDSF_Services_Framework_Admin_apigtw_int_dev@apigw-integration-DEV-eu-central-1'
alias kcait='get_apigw_kubeconfig.sh -c test -e int && kcx tls2_CDSF_Services_Framework_Admin_apigtw_int_test@apigw-integration-TEST-eu-central-1'
alias kcavd='get_apigw_kubeconfig.sh -c dev -e val && kcx tls2_CDSF_Services_Framework_Admin_apigtw_val_dev@apigw-validation-DEV-eu-central-1'
alias kcavt='get_apigw_kubeconfig.sh -c test -e val && kcx tls2_CDSF_Services_Framework_Admin_apigtw_val_test@apigw-validation-TEST-eu-central-1'
# GKE
alias kcg='kcx gke_gcp1002d-i3x5x6ms_europe-west9_cops-autopilot-cluster-1'
alias kcgm='kcx gke_gcp1002d-i3x5x6ms_europe-west3_cops-management-autopilot-cluster'

alias open_clusters_manager_argocd='/home/antoine/dev/oam.ci.clusters-manager/scripts/open_argocd_ui.sh -c prod'

alias get_virtualgtp_kubeconfig='/home/antoine/dev/oam.k8s.addons.virtualgtp/get_kubeconfig.sh'

alias kv='view_k8s_manifest'
function view_k8s_manifest () {
  temp_file=$(mktemp --suffix=.yaml)
  kubectl get -o yaml "$@" > "$temp_file"
  code "$temp_file"
}

function get_po_disruption_issue () {
  k get pdb -A -o json | jq -r '.items[] | select(.status.disruptionsAllowed == 0) | "\(.metadata.namespace)/\(.metadata.name)"'
}

function open_kubeflow_argocd() {
  kubectl --namespace argocd \
          --kubeconfig "${HOME}/.kube/kubeconfig_eks_kubeflow_integration_eu-central-1_Framework-admin.conf" \
          port-forward svc/argocd-server 9020:80 &>/dev/null &
}

function k_get_all() {
  # shellcheck disable=SC2068
  NAMES="$(kubectl api-resources \
                  --namespaced \
                  --verbs list \
                  -o name | tr '\n' ,)" \
  && export NAMES && kubectl get "${NAMES:0:-1}" --show-kind $@
}

function k_remove_finalizers() {
  kubectl patch $1 --type json --patch='[ { "op": "remove", "path": "/metadata/finalizers" } ]'
}

function k_force_remove_finalizers() {
  namespace=$1
  kubectl get ns "$namespace" -o json | jq '.spec = {"finalizers":[]}' | kubectl replace --raw "/api/v1/namespaces/$namespace/finalize" -f
}

function kg_composition_errors() {
  kubectl get "$1" "$2" -o=jsonpath='{.spec.resourceRef}{" "}{.spec.resourceRefs}' | jq '.[] | select( .name == null)'
}


function argocd_sync_app() {
  app=${1?}
  argocd login cd.argoproj.io --core
  argocd app terminate-op "$app"
  argocd app sync "$app"
}

function open_argocd() {
  env=${1:-integration}
  region=${2:-eu-central-1}
  cluster=${3:-applicative_k8s}

  case "$env" in
    integration)
      local_port=9001
      ;;
    validation)
      local_port=9002
      ;;
    production)
      case "$region" in
        eu-central-1)
          local_port=9003
          ;;
        us-east-1)
          local_port=9004
          ;;
      esac
      ;;
  esac

  echo "=> Launch ${env} ${region} argocd-server port-forward"
  kubectl --namespace argocd \
          port-forward svc/argocd-server $local_port:80 &>/dev/null &
          # --kubeconfig "${HOME}/.kube/kubeconfig_${cluster}_${env}_${region}-admin.conf" \
  echo "=> Wait for port-forward setup..."
  sleep 10
  echo "=> Open argo UI"
  google-chrome "http://localhost:$local_port/applications" &>/dev/null &
  echo "=> Current port-forward process"
  ps -ef | grep 'port-forward svc/argocd-server' | grep -v 'grep'
}

function enable_argocd() {
  kubectl scale sts,deploy --replicas 1 --all -n argocd
}

function disable_argocd() {
  kubectl scale sts,deploy --replicas 0 --all -n argocd
}

function kubeops() {
  echo "Searching kube config..."
  if [ -z "$1" ]; then
    if [[ -e $KUBECONFIG ]]; then 
      echo "No argument => loading current KUBECONFIG ($KUBECONFIG) !"
      CONFIGFILE=$(basename $KUBECONFIG)
    else
      echo "No KUBECONFIG : Either set KUBECONFIG env var or pass a kubeconfig name as argument"
      echo ""
      echo "Usage: ${0} <kube-config-name> "
      echo ""
      echo "------------------------------------"
      echo "List of available kube config :"
      echo "------------------------------------"
      find ${HOME}/.kube/ -maxdepth 1 -name "*config*" -exec sh -c 'basename "$1"' find-sh {} \;
      return
    fi
  else
    CONFIGFILE=${1}
  fi
  echo "Using kube config : ${CONFIGFILE}..."
  docker run -it --net=host -v "${HOME}:${HOME}" hjacobs/kube-ops-view --kubeconfig-path="${HOME}/.kube/${CONFIGFILE}"
}

function k() {
  kubectl "$@"
}

function kimg () {
  cat << EOF > "${HOME}/.kube/k8s-images.fmt"
NAMESPACE            NAME           IMAGE            INIT_IMAGE
metadata.namespace   metadata.name  spec.containers[0].image    spec.initContainers[0].image
EOF
  kubectl get pods -o custom-columns-file="${HOME}/.kube/k8s-images.fmt" "$@"
}

function apigw_dev() {
  configureAwsProfiles.sh --project-name infra --region eu-central-1 --environment integration --test none
  eksctl --profile target utils write-kubeconfig --cluster apigw-integration-DEV --region eu-central-1 --kubeconfig "${HOME}/.kube/dev/kubeconfig_apigw.conf"
  export KUBECONFIG=$HOME/.kube/dev/kubeconfig_apigw.conf
  AWS_DEFAULT_REGION=eu-central-1 AWS_PROFILE=target k cluster-info
}

function apigw_test() {
  configureAwsProfiles.sh --project-name infra --region eu-central-1 --environment integration --test none
  eksctl --profile target utils write-kubeconfig --cluster apigw-integration-TEST --region eu-central-1 --kubeconfig "${HOME}/.kube/test/kubeconfig_apigw.conf"
  export KUBECONFIG=$HOME/.kube/test/kubeconfig_apigw.conf
  AWS_DEFAULT_REGION=eu-central-1 AWS_PROFILE=target k cluster-info
}

function configure_env_from_current_apigw {
  REGION=$(kubectl get nodes -l alpha.eksctl.io/cluster-name -o jsonpath='{.items[0].metadata.labels.region}')
  export REGION
  ENVIRONMENT=$(kubectl get nodes -l alpha.eksctl.io/cluster-name -o jsonpath='{.items[0].metadata.labels.environment}')
  export ENVIRONMENT
  CLASSIFICATION=$(kubectl get nodes -l alpha.eksctl.io/cluster-name -o jsonpath='{.items[0].metadata.labels.classification}')
  export CLASSIFICATION
  export CLUSTER='api-gateway'
}

function configure_env_from_current_applicative_k8s {
  REGION=$(kubectl get nodes -l node-role.kubernetes.io/control-plane= -o jsonpath='{.items[0].metadata.annotations.region}')
  export REGION
  ENVIRONMENT=$(kubectl get nodes -l node-role.kubernetes.io/control-plane= -o jsonpath='{.items[0].metadata.annotations.role}' | tr '[:upper:]' '[:lower:]')
  export ENVIRONMENT
  CLASSIFICATION=$(kubectl get nodes -l node-role.kubernetes.io/control-plane= -o jsonpath='{.items[0].metadata.annotations.classification}' | tr '[:upper:]' '[:lower:]')
  export CLASSIFICATION
  export CLUSTER='applicative-k8s'
}

alias connect_applicative_k8s_node='/home/antoine/dev/oam.k8s.infra/k8s-scripts/send-ssh-public-key'
function kssh {
  # shellcheck disable=SC2001
  ip=$(echo "$1" | sed 's/ip-\([0-9]\{1,3\}-[0-9]\{1,3\}-[0-9]\{1,3\}-[0-9]\{1,3\}\).*/\1/')
  ip=${ip//-/.}
  ssh "$ip"
}

function convert_to_kustomize {
  dir=${1:=.}
  [[ -f ".version" ]] && source .version
  pushd "$dir" || return
  [[ -f "kustomization.yaml" ]] && rm "kustomization.yaml"
  [[ -d "result" ]] && rm -rf "result"
  kustomize create
  [[ $ADDON_NAMESPACE != "" ]] && kustomize edit set namespace "$ADDON_NAMESPACE"
  find . -type f -name '*.yaml' -not -path "./kustomization.yaml" -exec sh -c 'kustomize edit add resource "$1"' find-sh {} \;
  mkdir "result"
  kustomize build . -o result
  popd > /dev/null 2>&1 || true
}

############################################################
#                       Terraform
############################################################

function tfinit() {
  rm -rf ".terraform"
  mkdir ".terraform"
  cp "$HOME/.terraform.d/providers_cache/.terraform.lock.hcl" ".terraform"
  cp -R "$HOME/.terraform.d/providers_cache/providers" ".terraform"
  terraform init -reconfigure -get=true -upgrade=true
}

function get_tf_backend_s3_path() {
  bucket=$(grep -ohP 'bucket\s*=\s+"\K[^"]+' backend.tf)
  workspace_key_prefix=$(grep -ohP 'workspace_key_prefix\s*=\s+"\K[^"]+' backend.tf)
  key=$(grep -ohP 'key\s*=\s+"\K[^"]+' backend.tf)

  echo "s3://${bucket}/${workspace_key_prefix}/${TERRAFORM_WORKSPACE:=PROD}/${key}"
}
function show_tf_backend() {
  aws --profile backend s3 cp "$(get_tf_backend_s3_path)" -
}
function pull_tf_backend() {
  aws --profile backend s3 cp "$(get_tf_backend_s3_path)" "$(grep -ohP 'key\s*=\s+"\K[^"]+' backend.tf).json"
}
function push_tf_backend() {
  aws --profile backend s3 cp "$(get_tf_backend_s3_path)" "$(get_tf_backend_s3_path).backup" \
  && echo 'backup created !' \
  && aws --profile backend s3 cp "$(grep -ohP 'key\s*=\s+"\K[^"]+' backend.tf).json" "$(get_tf_backend_s3_path)"
}

##################################################################
#                          Certificates
##################################################################
function gencert() {
  mkdir -p /tmp/cert
  echo "test DNS command : dig -t txt +short _acme-challenge.${1} @8.8.8.8"
  docker run --rm -it --network=host -v /tmp/cert:/etc/letsencrypt certbot/certbot certonly --manual -d $1 -d \*.$1 --preferred-challenges dns && \
  sudo mv "${HOME}/.certs/${1}.pem" "${HOME}/.certs/${1}_$(date '+%Y%m%d').pem" | true && \
  sudo mv "${HOME}/.certs/${1}.key" "${HOME}/.certs/${1}_$(date '+%Y%m%d').key" | true && \
  sudo cp "/tmp/cert/live/${1}/fullchain.pem" "${HOME}/.certs/${1}.pem" && \
  sudo cp "/tmp/cert/live/${1}/privkey.pem" "${HOME}/.certs/${1}.key" && \
  sudo chown "${USER}:" "${HOME}/.certs/${1}."* && \
  sudo rm -rf /tmp/cert
  ls -lrt "${HOME}/.certs"
}

##################################################################
#                              SSH
##################################################################
function import_ssh_key() {
  if [[ "$1" == "" ]];
  then
    ls /media/sf_sharedfolder/.ssh
  else
    cp "/media/sf_sharedfolder/.ssh/$1" "${HOME}/.ssh"
    chmod 400 "${HOME}/.ssh/$1"
  fi
}

function write_public_ssh_key() {
  ssh-keygen -y -f "${HOME}/.ssh/$1" > "${HOME}/.ssh/${1%.pem}.pub"
  chmod 644 "${HOME}/.ssh/${1%.pem}.pub"
}

function set_ssh_config() {
  if [[ "$1" == "" ]];
  then
    ln -sf "${HOME}/.ssh/config_global" "${HOME}/.ssh/config"
  else
    ln -sf "${HOME}/.ssh/config_$1" "${HOME}/.ssh/config"
  fi
}

function manage_pgp() {
  gpg --list-secret-keys --keyid-format LONG
  echo "Create new key : vi ${HOME}/.gpg/config && gpg --batch --gen-key config"
  echo "Delete key : gpg --delete-secret-and-public-key --batch --yes XXXXXXXXXXXXXXXXX"
  echo "Import key : gpg --import ${HOME}/.gpg/xxxxxxxxxxx.asc"
  echo "Export public key : gpg --export --armor XXXXXXXXXXXXXXXXX > ${HOME}/.gpg/gpg_xxxxxxxxxxx_public.asc && chmod 644 ${HOME}/.gpg/gpg_xxxxxxxxxxx_public.asc"
  echo "Export private key : gpg --export-secret-keys --armor XXXXXXXXXXXXXXXXX > ${HOME}/.gpg/gpg_xxxxxxxxxxx.asc && chmod 400 ${HOME}/.gpg/gpg_xxxxxxxxxxx.asc"
}

#################################
# AWS
#################################

function check_aws_connection() {
  echo -en "${DEFAULT}Check AWS connection using '${1:=current}' profile : "
  [[ $1 != "" ]] && profile_param="--profile $1"
  # shellcheck disable=SC2086
  if eval "aws sts get-caller-identity $profile_param"
  then
    echo -e "${GREEN}OK"
    return 0
  else
    echo -e "${RED}KO"
    return 1
  fi
}

alias sfa='sso_login sfa 703253491819 1114-CDSF-sfa'
alias 1513='sso_login 1513 1513 1513-DevOps'
function sso_login() {
  unset AWS_PROFILE

  # We set tenant to avoid infinite loop issue
  # https://github.geo.conti.de/continental-cloud/aws-sso-cli/issues/20 is not fixed
  export SSO_TENANT="continental-ag-secure-cloud-access"

  # echo "\033[0;31m/!\\/!\\/!\\/!\\/!\\/!\\/!\\"
  # echo "\033[0;31m/!\\/!\\/!\\/!\\/!\\/!\\/!\\"
  # echo "\033[0;31m/!\\ DO NOT USE THE BROWSER OF YOU VM FOR AUTHENTICATION. USE THE ONE OF YOUR HOST"
  # echo "Usage: adl [profile] [region]"
  # echo "parameters are optional"
  # echo "\033[0;31m/!\\/!\\/!\\/!\\/!\\/!\\/!\\"
  # echo "\033[0;31m/!\\/!\\/!\\/!\\/!\\/!\\/!\\"
  aws-sso-cli configure browser none
  if [[ "$1" == "" ]];
  then
      aws-sso-cli login --tenant "$SSO_TENANT"
      # echo "Set AWS_PROFILE !"
  elif [[ $2 == "" && $3 == "" ]]
  then
      aws-sso-cli login --profile "$1" --tenant "$SSO_TENANT"
      # export AWS_PROFILE=$1
  else
      aws-sso-cli login --profile "$1" --account "$2" --role "$3" --tenant "$SSO_TENANT"
      # export AWS_PROFILE=$1
  fi

  check_aws_connection "$1" || echo "Bad credentials or clock drift, use 'clock_sync' to reset the clock"
}

function update_aws_sso_cli() {
  sudo -E curl -sL https://github.geo.conti.de/api/v3/repos/continental-cloud/aws-sso-cli/releases/latest \
  | grep browser_download_url \
  | grep linux \
  | cut -d '"' -f 4 \
  | xargs -n1 sudo curl -sL -o /usr/local/bin/aws-sso-cli \
  && sudo chmod 755 /usr/local/bin/aws-sso-cli \
  && sudo chown $UID /usr/local/bin/aws-sso-cli
  aws-sso-cli --version
}

function aws_decode-authorization-message() {
  aws sts decode-authorization-message --encoded-message $1 --query 'DecodedMessage' --output text | jq
}

function get_PrivateIpAddresses() {
  aws_profile_name='int'
  if [[ "$1" != "" ]];
  then
    aws_profile_name="$1"
  fi
  classification='PROD'
  if [[ "$2" != "" ]];
  then
    classification="$2"
  fi
  echo "---------------------------------------------------------"
  aws_account_id=$(aws --profile ${aws_profile_name} sts get-caller-identity --query "Account" --output text)
  echo "AWS account ID : ${aws_account_id}"
  case "$aws_account_id" in
      082461646758)
          aws_account_name="integration"
          ;;
      818436620314)
          aws_account_name="validation"
          ;;
      288932515620)
          aws_account_name="production"
          ;;
      517211653115)
          aws_account_name="log_management"
          ;;
      821840332237)
          aws_account_name="china"
          ;;
      *)
      echo "Unknown AWS account ID: ${aws_account_id}" >&2
  esac
  echo "AWS account : ${aws_account_name}"
  echo "AWS region : $(aws --profile ${aws_profile_name} configure get region)"
  echo "Classification : ${classification}"
  echo "---------------------------------------------------------"
  aws --profile ${aws_profile_name} ec2 describe-instances --filters Name=tag:Classification,Values=${classification} --query "Reservations[].Instances[].[Tags[?Key=='Application'].Value[] | [0], Tags[?Key=='Usage'].Value[] | [0], Tags[?Key=='Environment'].Value[] | [0], PrivateIpAddress, LaunchTime, Tags[?Key=='Deploy_User'].Value[] | [0]]" --output text | sort
}

function delete_test_dev_amis() {
  AWS_DEFAULT_REGION=eu-central-1 ${HOME}/dev/infra.builds.docker.aws-manager/launch.sh delete_old_ami_and_snapshots.sh --filters Name=tag:Classification,Values=TEST --keep-last 0
  AWS_DEFAULT_REGION=eu-central-1 ${HOME}/dev/infra.builds.docker.aws-manager/launch.sh delete_old_ami_and_snapshots.sh --filters Name=tag:Classification,Values=DEV --keep-last 0
}

#################################
# GCP
#################################
alias gcl='gcloud auth login --no-launch-browser && gcloud config set project gcp1002d-i3x5x6ms'
alias get_cops_management_kubeconfig='get_gke_kubeconfig cops-management-autopilot-cluster'
alias get_cops_kubeconfig='get_gke_kubeconfig cops-autopilot-cluster-1'

function get_gke_kubeconfig {
  CLUSTER_NAME=$1
  if [[ "$CLUSTER_NAME" == "" ]];
  then
    echo -e "${RED}Error, 1 argument required : CLUSTER_NAME"
    echo -e "${DEFAULT}------"
    gcloud container clusters list
    return 1
  fi
  export KUBECONFIG="${HOME}/.kube/kubeconfig_gcp_${CLUSTER_NAME}"
  [[ -f $KUBECONFIG ]] && rm "$KUBECONFIG"
  touch "$KUBECONFIG"
  gcloud container clusters get-credentials "${CLUSTER_NAME}" --region europe-west3 --project gcp1002d-i3x5x6ms
  # kubeon
  kubectl cluster-info
  kubectl get ns
}
